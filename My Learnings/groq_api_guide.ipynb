{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1287a0",
   "metadata": {},
   "source": [
    "# Using the Groq API in Python – A Complete Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373c5424",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Overview & Library Installation\n",
    "\n",
    "**Theory:**\n",
    "Groq offers a REST-based API that’s mostly compatible with the OpenAI Chat Completions interface. Fortunately, there’s an official Python client — `groq` — which wraps API calls in a clean, typed SDK.\n",
    "\n",
    "**Installation:**\n",
    "\n",
    "```bash\n",
    "pip install groq\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfc9e2b",
   "metadata": {},
   "source": [
    "## 2. Basic Usage (Synchronous)\n",
    "\n",
    "**Theory:** The `Groq` client simplifies authentication, request formatting, and response parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5eafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum entanglement in simple terms.\"}\n",
    "    ],\n",
    "    model=\"llama3-8b-8192\"\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468af46",
   "metadata": {},
   "source": [
    "## 3. Asynchronous Usage with Streaming\n",
    "**Theory:** If your workflow needs async operations (e.g., in web apps or event loops), use AsyncGroq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import asyncio\n",
    "from groq import AsyncGroq\n",
    "\n",
    "async def main():\n",
    "    client = AsyncGroq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "    response = await client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}],\n",
    "        model=\"llama3-70b-8192\",\n",
    "        stream=True\n",
    "    )\n",
    "    async for chunk in response:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98cceae",
   "metadata": {},
   "source": [
    "This streams content in real time, printing as tokens arrive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663bbfec",
   "metadata": {},
   "source": [
    "## 4. Advanced: Accessing Raw or Streaming HTTP Responses\n",
    "Sometimes you may want low-level control—inspect headers, stream manually, or handle undocumented features.\n",
    "\n",
    "**Features:**\n",
    "\n",
    "- .with_raw_response: Access HTTP headers or raw data.\n",
    "\n",
    "- .with_streaming_response: Stream response content manually via context managers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba836729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "with client.chat.completions.with_streaming_response.create(\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Hello?\"}],\n",
    "    model=\"llama3-8b-8192\"\n",
    ") as resp:\n",
    "    print(\"Response headers:\", resp.headers)\n",
    "    for line in resp.iter_lines():\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e9f683",
   "metadata": {},
   "source": [
    "Useful when you need resource tracking or custom streaming workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507129d",
   "metadata": {},
   "source": [
    "## 5. Integration with Code Interpreter (E2B)\n",
    "Want to build an app that generates code and runs it safely? Groq can pair with E2B Code Interpreter.\n",
    "\n",
    "**Setup Steps:**\n",
    "\n",
    "1. Install packages:\n",
    "\n",
    "`pip install groq e2b-code-interpreter python-dotenv`\n",
    "\n",
    "2. Set environment variables for API keys (GROQ_API_KEY, E2B_API_KEY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from e2b_code_interpreter import Sandbox\n",
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "client = Groq(api_key=os.environ.get('GROQ_API_KEY'))\n",
    "sbx = Sandbox()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-70b-versatile\",\n",
    "    messages=[{\"role\":\"system\",\"content\":\"Write Python to sum a list.\"}, {\"role\":\"user\",\"content\":\"Create code to sum [1,2,3,4,5].\"}]\n",
    ")\n",
    "code = response.choices[0].message.content.split(\"```python\")[1].split(\"```\")[0]\n",
    "\n",
    "print(\"Generating code:\\n\", code)\n",
    "print(\"Execution:\\n\", sbx.run_code(code).logs.stdout[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879b7c5",
   "metadata": {},
   "source": [
    "This dynamically generates and runs Python in a sandbox. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cf916a",
   "metadata": {},
   "source": [
    "## 6. Using Groq with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de8e69",
   "metadata": {},
   "source": [
    "If you're already using LangChain, integrating Groq is smooth thanks to shared API semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92978b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"your-key\"\n",
    "ai = ChatGroq(model=\"llama3-8b-8192\", temperature=0.7, max_tokens=512)\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"task\"], template=\"Write a Python program to {task}.\")\n",
    "chain = LLMChain(llm=ai, prompt=prompt)\n",
    "print(chain.run(task=\"calculate factorial\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbbf97b",
   "metadata": {},
   "source": [
    "Leverage existing LangChain patterns with Groq backend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb47fc",
   "metadata": {},
   "source": [
    "\n",
    "## Summary Table\n",
    "\n",
    "| Task                      | Example Usage                                     |\n",
    "| ------------------------- | ------------------------------------------------- |\n",
    "| Install library           | `pip install groq`                                |\n",
    "| Sync Chat Completion      | `Groq(...).chat.completions.create(…)`            |\n",
    "| Async + Streaming         | `AsyncGroq(...)` + `stream=True`                  |\n",
    "| Raw/Streaming HTTP access | `.with_raw_response` / `.with_streaming_response` |\n",
    "| Code generation + sandbox | Groq + E2B code interpreter                       |\n",
    "| LangChain integration     | `langchain_groq.ChatGroq`                         |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
