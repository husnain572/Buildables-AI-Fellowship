{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6b55c5",
   "metadata": {},
   "source": [
    "## **Conversational AI Concepts & Model Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b7ace2",
   "metadata": {},
   "source": [
    "Agenda of this week 2 is:\n",
    "\n",
    "- Understand LLMs, STT, TTS models and their roles.\n",
    "\n",
    "- Know how to connect to LLMs with APIs (Groq as example).\n",
    "\n",
    "- Use Python (requests + JSON) for API interaction.\n",
    "\n",
    "- Start building a basic chatbot with memory and preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c5144",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03968dc4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Question 1**: What is an LLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd894fc",
   "metadata": {},
   "source": [
    "üëâ It‚Äôs like a super-smart text predictor that can read, understand, and generate human-like sentences.\n",
    "\n",
    "You give it some words ‚Üí it guesses the next words in a way that makes sense.\n",
    "\n",
    "For example:\n",
    "\n",
    "1) You ask a question ‚Üí it gives you an answer.\n",
    "\n",
    "2) You write a sentence ‚Üí it can complete it.\n",
    "\n",
    "3) You give it a topic ‚Üí it can write an essay, code, or even a story.\n",
    "\n",
    "So, its a type of AI trained on huge amounts of text data to generate or understand text.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77076ddc",
   "metadata": {},
   "source": [
    "### Types of LLMs\n",
    "\n",
    "1. Encoder-only models (e.g., BERT)\n",
    "\n",
    "    - Best for understanding text (classification, sentiment analysis, embeddings).\n",
    "\n",
    "    - Not good at generating text.\n",
    "\n",
    "2. Decoder-only models (e.g., GPT, LLaMA, Mistral)\n",
    "\n",
    "    - Best for text generation (chatbots, writing, summarization).\n",
    "\n",
    "    - What we use in chatbots.\n",
    "\n",
    "3. Encoder-decoder models (e.g., T5, BART)\n",
    "\n",
    "    - Good at transforming text (translation, summarization, Q&A)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339099fe",
   "metadata": {},
   "source": [
    "### Must-Knows about LLMs\n",
    "\n",
    "- They don‚Äôt ‚Äúthink‚Äù like humans ‚Üí They predict text based on training.\n",
    "\n",
    "- Garbage in ‚Üí garbage out: Poor prompts = poor answers.\n",
    "\n",
    "- Token limits: Models can only ‚Äúsee‚Äù a certain number of words at a time.\n",
    "\n",
    "- Biases: Trained on internet text ‚Üí may reflect biases/errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b2dd4",
   "metadata": {},
   "source": [
    "### **Quick Question**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753565a",
   "metadata": {},
   "source": [
    "1. Why might a chatbot built on BERT (encoder-only) struggle to answer open-ended questions?\n",
    "\n",
    "- BERT stands for Bidirectional Encoder Representations from Transformers. It is an encoder-only transformer model. BERT works by understanding the context of the input text and then producing an output. Basically, every word in a sentence looks at the words before and after it to understand its meaning. In this way, we get a context-aware representation of the sentence.\n",
    "\n",
    "- In short, BERT is not used to generate content but rather for tasks like text classification, named entity recognition, and extractive question answering. For example, if I want to know the sentiment of the sentence \"I love ice cream!\", BERT will read the entire sentence, understand the meaning of each word in relation to the others, and predict the overall sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5feffca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Speech-to-Text (STT) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393abf7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Question 2**: What is STT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54ac00",
   "metadata": {},
   "source": [
    "üëâ listens to your voice and turns it into written text.\n",
    "\n",
    "- Converts **audio ‚Üí text**.\n",
    "- Enables voice input for conversational AI.\n",
    "- Think of it as the **ears** of the chatbot.\n",
    "\n",
    "**Popular STT Models**:\n",
    "\n",
    "1) **Whisper (OpenAI)** ‚Äì strong at multilingual speech recognition.\n",
    "2) **Google Speech-to-Text API** ‚Äì widely used, real-time transcription.\n",
    "3) **Vosk** ‚Äì lightweight, offline speech recognition.\n",
    "\n",
    "**Common Usages**\n",
    "\n",
    "1) Voice assistants (Alexa, Siri, Google Assistant).\n",
    "2) Automated captions in meetings or lectures.\n",
    "3) Voice-enabled customer support.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc99714c",
   "metadata": {},
   "source": [
    "### Must-Knows about STT\n",
    "\n",
    "- Accuracy depends on **noise, accents, clarity of speech**.\n",
    "\n",
    "- Some models need **internet connection** (API-based), others run **offline**.\n",
    "\n",
    "- Preprocessing audio (noise reduction) improves results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23bf9a",
   "metadata": {},
   "source": [
    "### **Quick Questions**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407d8a82",
   "metadata": {},
   "source": [
    "2. Why do you think meeting transcription apps like Zoom or Google Meet struggle when multiple people talk at once?\n",
    "\n",
    "- Speech-to-text models used in apps like Zoom or Google Meet struggle in this case because automatic speech recognition (ASR) systems are typically designed to process one speaker‚Äôs voice at a time. When multiple people speak simultaneously, their voices overlap, creating what‚Äôs known as the \"cocktail party problem\". This makes it harder for the model to separate and identify each individual‚Äôs speech, leading to reduced accuracy in transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2959a81",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Text-to-Speech (TTS) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6650b62d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Question 3**: What is TTS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99448e",
   "metadata": {},
   "source": [
    "üëâ takes written text and speaks it out loud in a human-like voice.\n",
    "\n",
    "- Converts **text ‚Üí audio (speech)**.\n",
    "- Think of it as the **mouth** of the chatbot.\n",
    "- Makes AI ‚Äúspeak‚Äù naturally.\n",
    "\n",
    "**Popular TTS Models**:\n",
    "\n",
    "1) **Google TTS** ‚Äì supports many languages and voices.\n",
    "2) **Amazon Polly** ‚Äì lifelike voice synthesis with customization.\n",
    "3) **ElevenLabs** ‚Äì cutting-edge, realistic voice cloning.\n",
    "\n",
    "**Common Usages**\n",
    "\n",
    "1) Screen readers for visually impaired users.\n",
    "2) AI chatbots with voice output.\n",
    "3) Audiobooks or podcast generation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdb2471",
   "metadata": {},
   "source": [
    "### Must-Knows about TTS\n",
    "\n",
    "- Some voices sound robotic; others use **neural TTS** for natural tones.\n",
    "\n",
    "- Latency matters ‚Üí If too slow, conversation feels unnatural.\n",
    "\n",
    "- Some TTS services allow **custom voices**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49cb51",
   "metadata": {},
   "source": [
    "### **Quick Questions**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f3eb2",
   "metadata": {},
   "source": [
    "3. If you were designing a voice-based AI tutor, what qualities would you want in its TTS voice (tone, speed, clarity, etc.)?\n",
    "\n",
    "- If I get the opportunity to design a voice-based AI tutor, I will focus mainly on accent and tone because tone helps classify whether it‚Äôs a human or a machine. That‚Äôs why I will definitely work on accent. Secondly, speed and clarity also matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8042c582",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Using APIs for LLMs with Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45adfbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install groq\n",
    "# pip install groq python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7889d8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imran Khan is a Pakistani politician, former cricketer, and philanthropist who served as the 22nd Prime Minister of Pakistan from August 2018 to April 2022. Here are some key points about his life and career:\n",
      "\n",
      "**Early Life and Education**\n",
      "\n",
      "Imran Ahmed Khan Niazi was born on October 5, 1952, in Lahore, Punjab, Pakistan. His father, Ikramullah Khan Niazi, was a civil engineer, and his mother, Shaukat Khanum, was a housewife. Imran Khan studied at Aitchison College in Lahore and later graduated from Keble College, Oxford, with a degree in Philosophy, Politics, and Economics.\n",
      "\n",
      "**Cricketer and Philanthropist**\n",
      "\n",
      "Imran Khan is widely regarded as one of the greatest cricketers of all time. He played for the Pakistani national team from 1971 to 1992 and led the team to victory in the 1992 Cricket World\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello! Tell me about Imran Khan?\"}],\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c20eb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc2ce6",
   "metadata": {},
   "source": [
    "### Assignment 1: LLM Understanding\n",
    "\n",
    "* Write a short note (3‚Äì4 sentences) explaining the difference between **encoder-only, decoder-only, and encoder-decoder LLMs**.\n",
    "* Give one example usage of each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58d5ff",
   "metadata": {},
   "source": [
    "**Encoder-only:**\n",
    "Encoder-only models use encoder part of the transformer architecture. They are designed to understand the input, not to generate new text. They are great for the tasks that need the model to make sense of a sentence or document, like text classification or extracting information.\n",
    "**Use Cases:**\n",
    "- Sentiment Analysis\n",
    "- Text Classisfication\n",
    "- Named Entity Recognition\n",
    "- Extractive Question Answering\n",
    "\n",
    "**Decoder-Only Models:**\n",
    "These models use only the decoder part of the transformer architechture. They are use for generating text or predicting the next word in a sequence. These models are great for tasks like writing stories or auto-completing sentences. \n",
    "**Use Cases:**\n",
    "- Text Generation\n",
    "- Language Modeling\n",
    "\n",
    "**Encoder-Decoder Models:**\n",
    "These models use both encoder and decoder part of the transformer. The encoder first undderstands the input, and the decoder then generates a related output. This is useful for tasks where you need to transform the input into something else, like translating a sentence into another language or summarizing text. \n",
    "**Use Cases:**\n",
    "- Question Answering\n",
    "- Text Summarization\n",
    "- Machine Translation\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370084b8",
   "metadata": {},
   "source": [
    "### Assignment 2: STT/TTS Exploration\n",
    "\n",
    "* Find **one STT model** and **one TTS model** (other than Whisper/Google).\n",
    "* Write down:\n",
    "\n",
    "  * What it does.\n",
    "  * One possible application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688ed20",
   "metadata": {},
   "source": [
    "**STT Model:**\n",
    "\n",
    "\n",
    "Granite Speech 3.3 is an STT text model developed by IBM, targeting enterprises. With 8 billion parameters, it‚Äôs the largest open-source STT model. It is designed around popular languages used by businesses. Granite Speech 3.3 is optimized for English, French, German, and Spanish, and also excels at English-to-Japanese and English-to-Mandarin with built-in configurable speech translations. This makes Granite Speech 3.3 ideal for applications that require multi-lingual and translation support. \n",
    "\n",
    "**For example**, Granite Speech 3.3 can be easily configure to transcribe an English instructional video into Mandarin characters.\n",
    "\n",
    "**TTS Model:**\n",
    "\n",
    "Higgs Audio V2 is a massive TTS model developed by BosonAI. It‚Äôs currently the top trending text-to-speech model on Hugging Face. It‚Äôs an open sourced model that was built on top of Llama 3.2 3B, pre-trained on over 10 million hours of audio data. This model provides industry-leading expressive audio generation and multilingual voice cloning. \n",
    "\n",
    "**For example**, Higgs Audio V2 wins audience scores on emulating emotion and question-asking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84824b65",
   "metadata": {},
   "source": [
    "### Assignment 3: Build a Chatbot with Memory\n",
    "\n",
    "* Write a Python program that:\n",
    "\n",
    "  * Takes user input in a loop.\n",
    "  * Sends it to Groq API.\n",
    "  * Stores the last 5 messages in memory.\n",
    "  * Ends when user types `\"quit\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2e0a69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:Hello, kesy ho?\n",
      "Bot: Namaste. Main theek hoon, dhanyavad. Aap kaise ho?\n",
      "You:Main bhi theek hun bhai\n",
      "Bot: Achha hai! Kya karna hai? Chahte hain koi baat karne mein madad karna?\n",
      "You:kuch bhi nahi bas, testing kr rha hun \n",
      "Bot: Testing karna bahut zaroori hai. Main thoda sa test kar raha hoon bhi, aapka conversation ka response dekhna. Agar koi problem to kuch nahi hai, aapko kuch bhi pata nahi chal raha hai?\n",
      "You:quit\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
    "client=Groq(api_key=GROQ_API_KEY) \n",
    "messages=[]\n",
    "while True:\n",
    "    user_input = input(\"Type your message: \")\n",
    "    print(f\"You:{user_input}\",end=\"\\n\")\n",
    "    if(user_input.lower()==\"quit\"):\n",
    "        break\n",
    "    messages.append({\"role\":\"user\",\"content\":user_input})\n",
    "    messages=messages[-10:]\n",
    "    response=client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=messages,\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    reply=response.choices[0].message.content\n",
    "    messages.append({\"role\":\"assistant\",\"content\":reply})\n",
    "    print(f\"Bot: {reply}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef3132",
   "metadata": {},
   "source": [
    "### Assignment 4: Preprocessing Function\n",
    "\n",
    "* Write a function to clean user input:\n",
    "\n",
    "  * Lowercase text.\n",
    "  * Remove punctuation.\n",
    "  * Strip extra spaces.\n",
    "\n",
    "Test with: `\"  HELLo!!!  How ARE you?? \"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abd9fa5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are you'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text=text.lower()\n",
    "    text=text.strip()\n",
    "    text=''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    text=' '.join(text.split())\n",
    "    return text\n",
    "clean_text(\"  HELLo!!!  How ARE you?? \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53027998",
   "metadata": {},
   "source": [
    "### Assignment 5: Text Preprocessing\n",
    "\n",
    "* Write a function that:\n",
    "\n",
    "    * Converts text to lowercase.\n",
    "    * Removes punctuation & numbers.\n",
    "    * Removes stopwords (`the, is, and...`).\n",
    "    * Applies stemming or lemmatization.\n",
    "    * Removes words shorter than 3 characters.\n",
    "    * Keeps only nouns, verbs, and adjectives (using POS tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4cc05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72afa501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'running', 'coding']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.data.path.append('/root/nltk_data')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub(r'[^a-zA-Z\\s]','',text)\n",
    "    text_tokens=word_tokenize(text)\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    text=[word for word in text_tokens if word not in stop_words]\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    text=[lemmatizer.lemmatize(word) for word in text]\n",
    "    text=[word for word in text if len(word)>=3]\n",
    "    tagged_words=pos_tag(text)\n",
    "    allowed_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS']\n",
    "    filtered_words=[word for word, tag in tagged_words if tag in allowed_tags]\n",
    "    return filtered_words\n",
    "\n",
    "\n",
    "print(preprocess_text(\"  HELLo!!!  How ARE you?? I am running fast and coding happily. \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb68c035",
   "metadata": {},
   "source": [
    "### Assignment 6: Reflection\n",
    "\n",
    "* Answer in 2‚Äì3 sentences:\n",
    "\n",
    "    * Why is context memory important in chatbots?\n",
    "    * Why should beginners always check **API limits and pricing**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4c903",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Context memory in chatbots** is important because it enables coherent, multi-turn conversations by remembering past interactions. This leads to more personalized, efficient, and satisfying user experiences. Without context memory, chatbots act as **stateless** systems, treating each prompt as a new conversation.\n",
    "\n",
    "**Rate limiting** is essential for APIs because all APIs operate on finite resources. It improves service availability for as many users as possible by preventing excessive resource usage.\n",
    "\n",
    "**Why API Pricing is Important:**\n",
    "\n",
    "1. **Cost Control** ‚Äì Prevents unexpected high bills by knowing usage charges in advance.\n",
    "2. **Efficient Design** ‚Äì Encourages minimizing API calls/tokens to save money.\n",
    "3. **Scaling Decisions** ‚Äì Helps plan affordable solutions as usage grows.\n",
    "4. **Client Billing** ‚Äì Allows accurate cost estimates for clients or projects.\n",
    "5. **Provider Comparison** ‚Äì Enables choosing the best value API for performance vs cost.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b787de4",
   "metadata": {},
   "source": [
    "### **Hints:**\n",
    "\n",
    "1) Stemming:\n",
    "    - Cuts off word endings to get the ‚Äúroot.‚Äù\n",
    "    - Very mechanical ‚Üí may produce non-real words.\n",
    "    - Example:\n",
    "        - \"studies\" ‚Üí \"studi\"\n",
    "        - \"running\" ‚Üí \"run\"\n",
    "\n",
    "2) Lemmatization:\n",
    "    - Smarter ‚Üí uses vocabulary + grammar rules.\n",
    "    - Always gives a real word (the **lemma**).\n",
    "    - Example:\n",
    "        - \"studies\" ‚Üí \"study\"\n",
    "        - \"running\" ‚Üí \"run\"\n",
    "\n",
    "3) Part-of-Speech (POS) tagging means labeling each word in a sentence with its grammatical role ‚Äî like **noun, verb, adjective, adverb, pronoun, etc.**\n",
    "\n",
    "    - Example:\n",
    "        - Sentence ‚Üí *‚ÄúThe cat is sleeping on the mat.‚Äù*\n",
    "\n",
    "    - POS tags ‚Üí\n",
    "        - The ‚Üí Determiner (DT)\n",
    "        - cat ‚Üí Noun (NN)\n",
    "        - is ‚Üí Verb (VBZ)\n",
    "        - sleeping ‚Üí Verb (VBG)\n",
    "        - on ‚Üí Preposition (IN)\n",
    "        - the ‚Üí Determiner (DT)\n",
    "        - mat ‚Üí Noun (NN)\n",
    "\n",
    "    - **In short:** POS tagging helps machines understand **how words function in a sentence**, which is useful in NLP tasks like machine translation, text classification, and question answering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec98bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Recap\n",
    "\n",
    "This week I learned:\n",
    "\n",
    "* **LLMs**: Types, uses, must-knows.\n",
    "* **STT & TTS**: How they connect with LLMs.\n",
    "* **APIs**: Connecting to LLMs with Groq.\n",
    "* Built your first chatbot foundation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fellowship_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
