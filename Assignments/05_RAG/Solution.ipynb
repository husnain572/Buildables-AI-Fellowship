{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d161d88",
   "metadata": {},
   "source": [
    "## What is RAG ?\n",
    "RAG is a technique where a language model first looks up (retrieves) useful information from a database or documents, and then uses that information to give a better answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58c3da",
   "metadata": {},
   "source": [
    "## Why and when we prefer RAG over finetuning ?\n",
    "We **prefer RAG over finetuning** when: We want the model to give **up-to-date or specific answers** from **our own data** without changing the model itself.\n",
    "\n",
    "---\n",
    "\n",
    "**Why prefer RAG?**\n",
    "\n",
    "* **Cheaper & faster** – No need to train the model again.\n",
    "* **Easier to update** – Just change the documents, not the model.\n",
    "* **Better for private or large data** – You keep data separate and safe.\n",
    "\n",
    "---\n",
    "\n",
    "**When to use RAG?**\n",
    "\n",
    "* When your data **changes often** (like news, product lists).\n",
    "* When you want the model to **answer from your documents**.\n",
    "* When **training a model is too costly or slow**.\n",
    "\n",
    "---\n",
    "\n",
    "Think of RAG like **giving the model a library to read** instead of teaching it everything from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b6ec9f",
   "metadata": {},
   "source": [
    "# Install Libraries\n",
    "* **`langchain`** – Core framework to build LLM-powered applications.\n",
    "* **`langchain-community`** – Extra integrations like tools, APIs, and vector stores.\n",
    "* **`langchain-pinecone`** – Connects LangChain with Pinecone for vector storage and retrieval.\n",
    "* **`langchain_groq`** – Enables LangChain to use Groq's ultra-fast language models.\n",
    "* **`datasets`** – Provides ready-to-use NLP/ML datasets from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ec83ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain==0.3.23 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-community==0.3.21 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-pinecone==0.2.5 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (0.2.5)\n",
      "Requirement already satisfied: langchain_groq in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (0.3.8)\n",
      "Requirement already satisfied: datasets==3.5.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (3.5.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain==0.3.23) (0.3.78)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain==0.3.23) (0.3.11)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain==0.3.23) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain==0.3.23) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain==0.3.23) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain==0.3.23) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain==0.3.23) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-community==0.3.21) (3.10.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-community==0.3.21) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-community==0.3.21) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-community==0.3.21) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-community==0.3.21) (0.4.2)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-community==0.3.21) (1.26.4)\n",
      "Requirement already satisfied: pinecone<7.0.0,>=6.0.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (6.0.2)\n",
      "Requirement already satisfied: langchain-tests<1.0.0,>=0.3.7 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-pinecone==0.2.5) (0.3.22)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from datasets==3.5.0) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from datasets==3.5.0) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from datasets==3.5.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from datasets==3.5.0) (2.3.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from datasets==3.5.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from datasets==3.5.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from datasets==3.5.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets==3.5.0) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from datasets==3.5.0) (0.35.3)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from datasets==3.5.0) (25.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (6.7.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain==0.3.23) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain==0.3.23) (4.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.51->langchain==0.3.23) (3.0.0)\n",
      "Requirement already satisfied: pytest<9.0.0,>=7.0.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (8.4.2)\n",
      "Requirement already satisfied: pytest-asyncio<2.0.0,>=0.20.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (1.2.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (0.28.1)\n",
      "Requirement already satisfied: syrupy<5.0.0,>=4.0.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (4.9.1)\n",
      "Requirement already satisfied: pytest-socket<1.0.0,>=0.7.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (0.7.0)\n",
      "Requirement already satisfied: pytest-benchmark in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (5.1.0)\n",
      "Requirement already satisfied: pytest-codspeed in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (4.1.1)\n",
      "Requirement already satisfied: pytest-recording in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (0.13.4)\n",
      "Requirement already satisfied: vcrpy<8.0.0,>=7.0.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (7.0.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (4.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (0.16.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.23) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.23) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain==0.3.23) (0.23.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (0.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (2.5.0)\n",
      "\u001b[33mWARNING: pinecone 6.0.2 does not provide the extra 'async'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.23) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.23) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==0.3.23) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.21) (1.1.1)\n",
      "Requirement already satisfied: iniconfig>=1 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pytest<9.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pytest<9.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pytest<9.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2.19.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.3.23) (3.4.3)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.3.23) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.21) (1.1.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from vcrpy<8.0.0,>=7.0.0->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (1.17.3)\n",
      "Requirement already satisfied: propcache>=0.2.1 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.21) (0.4.1)\n",
      "Requirement already satisfied: groq<1,>=0.30.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from langchain_groq) (0.31.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from groq<1,>=0.30.0->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from groq<1,>=0.30.0->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets==3.5.0) (1.1.10)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from python-dateutil>=2.5.3->pinecone<7.0.0,>=6.0.0->pinecone[async]<7.0.0,>=6.0.0->langchain-pinecone==0.2.5) (1.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pandas->datasets==3.5.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pandas->datasets==3.5.0) (2025.2)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (9.0.0)\n",
      "Requirement already satisfied: cffi>=1.17.1 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2.0.0)\n",
      "Requirement already satisfied: rich>=13.8.1 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (14.1.0)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from cffi>=1.17.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (2.23)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain-pinecone==0.2.5) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain==0.3.23 langchain-community==0.3.21 langchain-pinecone==0.2.5 langchain_groq datasets==3.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e32146",
   "metadata": {},
   "source": [
    "## Load API Keys from .env file\n",
    "### What is env file?\n",
    "\n",
    "* A .env file is a simple text file that stores environment variables (like API keys and secrets) in key=value format.\n",
    "* Example content of a .env file:\n",
    "* PINECONE_API_KEY=your_pinecone_api_key_here\n",
    "* GROQ_API_KEY=your_groq_api_key_here\n",
    "\n",
    "* It helps keep sensitive information out of your code and makes it easier to manage secrets securely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049523f",
   "metadata": {},
   "source": [
    "**Imports tools** to:\n",
    "\n",
    "* Use environment variables (`os`)\n",
    "* Load values from a `.env` file (`load_dotenv`)\n",
    "* **os** is a Python built-in module that lets your code interact with the operating system (like Windows, macOS, Linux).\n",
    "---\n",
    "* **Loads the `.env` file** so Python can use the secret keys stored in it (like API keys).\n",
    "* **Gets the values** of `PINECONE_API_KEY` and `GROQ_API_KEY` from the `.env` file.\n",
    "* **In Short:** This code **reads your secret keys from a `.env` file** so you don’t have to write them directly in your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9683e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ API is loaded successfully.\n",
      "Pinecone is loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "pinecone_api=os.getenv(\"PINECONE_API_KEY\")\n",
    "groq_api=os.getenv(\"FELLOWSHIP_GROQ_KEY\")\n",
    "\n",
    "if groq_api:\n",
    "    print(\"GROQ API is loaded successfully.\")\n",
    "else:\n",
    "    print(\"GROQ API is not loaded.\")\n",
    "if pinecone_api:\n",
    "    print(\"Pinecone is loaded successfully.\")\n",
    "else:\n",
    "    print(\"Pinecone API is not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2244cfa2",
   "metadata": {},
   "source": [
    "## What is `langchain_groq`?\n",
    "\n",
    "`langchain_groq` is a **LangChain integration** that lets you **connect to Groq’s LLMs** (like LLaMA3) easily.\n",
    "\n",
    "Think of it as a **bridge between LangChain and Groq’s fast language models**.\n",
    "\n",
    "---\n",
    "\n",
    "## What is `ChatGroq`?\n",
    "\n",
    "`ChatGroq` is a **class (tool)** inside `langchain_groq`.\n",
    "\n",
    "It lets you:\n",
    "\n",
    "* **Send prompts** to Groq-hosted models\n",
    "* **Receive responses** from those models\n",
    "* Use these models in your **LangChain app**, like chatbots, RAG, agents, etc.\n",
    "\n",
    "---\n",
    "\n",
    "**Why do we use this?**\n",
    "\n",
    "Instead of manually setting up HTTP requests to Groq’s API, `ChatGroq` makes it **super easy**:\n",
    "\n",
    "* Lets you talk to a specific Groq model (`llama3-8b-8192`)\n",
    "* Works smoothly with LangChain tools (retrievers, chains, memory, etc.)\n",
    "* Connects securely with your `groq_api_key`\n",
    "\n",
    "---\n",
    "\n",
    "#### In Simple Words\n",
    "\n",
    "* `langchain_groq` lets LangChain talk to Groq.\n",
    "* `ChatGroq` is the tool that helps you **chat with Groq’s AI model** using your API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34e5c13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/fellowship_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "chat=ChatGroq(\n",
    "    groq_api_key=groq_api,\n",
    "    model_name=\"Llama-3.3-70B-Versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e2f9f",
   "metadata": {},
   "source": [
    "Groq uses the **same chat structure as OpenAI** because it runs **OpenAI-compatible models** like `llama3`, `mixtral`, etc.\n",
    "So just like OpenAI, chats with Groq **typically look like this in plain text**:\n",
    "\n",
    "```\n",
    "System: You are a helpful assistant.\n",
    "User: Hi, how are you?\n",
    "Assistant: I'm doing well! How can I assist you today?\n",
    "User: What is quantum computing?\n",
    "Assistant:\n",
    "```\n",
    "\n",
    "The final `\"Assistant:\"` without a response is what would prompt the model to continue the conversation. In the official OpenAI `ChatCompletion` endpoint these would be passed to the model in a format like:\n",
    "\n",
    "---\n",
    "\n",
    "**In Code (OpenAI/Groq-compatible format):**\n",
    "\n",
    "When using the API (like with `ChatGroq` or `ChatOpenAI` in LangChain), you use this structure:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"I'm doing well! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is quantum computing?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**In LangChain (message objects):**\n",
    "\n",
    "LangChain wraps those into **message classes**, like:\n",
    "\n",
    "```\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi, how are you?\"),\n",
    "    AIMessage(content=\"I'm doing well! How can I assist you today?\"),\n",
    "    HumanMessage(content=\"What is quantum computing?\")\n",
    "]\n",
    "```\n",
    "The format is very similar, we're just swapped the role of `\"user\"` for `HumanMessage`, and the role of `\"assistant\"` for `AIMessage`.\n",
    "\n",
    "Then you pass them to the model:\n",
    "\n",
    "```\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab9bf6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (SystemMessage, AIMessage, HumanMessage)\n",
    "messages=[\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Who is the founder of Pakistan?\"),\n",
    "    AIMessage(content=\"Quaid-e-Azam Muhammad Ali Jinnah was the founder of Pakistan.\"),\n",
    "    HumanMessage(content=\"I'd like to know about the history of Pakistan.\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe684b6",
   "metadata": {},
   "source": [
    "We generate the next response from the AI by passing these messages to the `ChatGroq` object.\n",
    "\n",
    "Like saying to the AI:\n",
    "\n",
    "“Here’s what has been said so far — now tell me what the AI should say next.”\n",
    "\n",
    "LangChain then handles formatting and sending this to the LLM backend, and res stores the AI’s next reply.\n",
    "\n",
    "**In Short:**\n",
    "* You define a conversation (via messages).\n",
    "* Call the LLM using chat(messages).\n",
    "* Get a response back — stored in res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37a14227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/09/vzgsbj512kgb6x26915zbtww0000gn/T/ipykernel_12755/2783252951.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res=chat(messages)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The history of Pakistan is a rich and complex one, spanning thousands of years. Here's a brief overview:\\n\\n**Ancient Civilizations (3300 BCE - 500 CE):**\\nThe region that is now Pakistan was home to some of the world's oldest and most influential civilizations, including the Indus Valley Civilization (3300-1300 BCE), the Vedic Civilization (1500-500 BCE), and the ancient Gandhara Civilization (500 BCE-500 CE). These civilizations made significant contributions to art, architecture, literature, and philosophy.\\n\\n**Islamic Conquest and Rule (711-1858 CE):**\\nIn 711 CE, Arab Muslims conquered the region, introducing Islam and establishing the Umayyad Caliphate. Over the centuries, various Muslim dynasties, including the Ghaznavids, Ghorids, and Mughals, ruled the region, leaving a lasting legacy in architecture, art, and culture.\\n\\n**British Colonial Rule (1858-1947 CE):**\\nIn 1858, the British East India Company established colonial rule in the region, which became part of British India. During this period, the region was divided into provinces, including Punjab, Sindh, Balochistan, and the North-West Frontier Province (now Khyber Pakhtunkhwa).\\n\\n**Pakistan Movement (1930s-1947 CE):**\\nIn the early 20th century, the Pakistan Movement, led by Muhammad Ali Jinnah, gained momentum. The movement sought to create a separate homeland for Muslims in British India, where they could live according to their own customs, laws, and values. On August 14, 1947, Pakistan gained independence, with Jinnah as its first Governor-General.\\n\\n**Early Years of Pakistan (1947-1958 CE):**\\nAfter independence, Pakistan faced numerous challenges, including the migration of millions of Muslims from India, the establishment of a new government and infrastructure, and the integration of various provinces and princely states. The country's first constitution was adopted in 1956, but it was short-lived, as a military coup in 1958 led to the establishment of a military dictatorship.\\n\\n**Military Rule and Democracy (1958-1971 CE):**\\nPakistan experienced periods of military rule, including the regimes of Ayub Khan (1958-1969) and Yahya Khan (1969-1971). In 1970, general elections were held, which led to the establishment of a democratic government under Zulfikar Ali Bhutto.\\n\\n**Bangladesh Liberation War (1971 CE):**\\nIn 1971, East Pakistan (now Bangladesh) declared independence, leading to a brutal civil war. The war ended with the surrender of Pakistani forces and the establishment of Bangladesh as a separate country.\\n\\n**Modern Pakistan (1972 CE-present):**\\nSince 1972, Pakistan has experienced periods of democracy and military rule, with various governments and leaders shaping the country's politics, economy, and society. Today, Pakistan is a nuclear-armed state with a diverse economy, a growing middle class, and a rich cultural heritage.\\n\\nThis is a brief overview of Pakistan's history. If you'd like to know more about a specific period or topic, feel free to ask!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 662, 'prompt_tokens': 86, 'total_tokens': 748, 'completion_time': 1.607331732, 'prompt_time': 0.004336049, 'queue_time': 0.048791581, 'total_time': 1.611667781}, 'model_name': 'Llama-3.3-70B-Versatile', 'system_fingerprint': 'fp_9e1e8f8435', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--89be9b5c-c975-4e50-a3d8-a2d15e5af83c-0', usage_metadata={'input_tokens': 86, 'output_tokens': 662, 'total_tokens': 748})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=chat(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d571ee3e",
   "metadata": {},
   "source": [
    "To see the models reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2757ef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history of Pakistan is a rich and complex one, spanning thousands of years. Here's a brief overview:\n",
      "\n",
      "**Ancient Civilizations (3300 BCE - 500 CE):**\n",
      "The region that is now Pakistan was home to some of the world's oldest and most influential civilizations, including the Indus Valley Civilization (3300-1300 BCE), the Vedic Civilization (1500-500 BCE), and the ancient Gandhara Civilization (500 BCE-500 CE). These civilizations made significant contributions to art, architecture, literature, and philosophy.\n",
      "\n",
      "**Islamic Conquest and Rule (711-1858 CE):**\n",
      "In 711 CE, Arab Muslims conquered the region, introducing Islam and establishing the Umayyad Caliphate. Over the centuries, various Muslim dynasties, including the Ghaznavids, Ghorids, and Mughals, ruled the region, leaving a lasting legacy in architecture, art, and culture.\n",
      "\n",
      "**British Colonial Rule (1858-1947 CE):**\n",
      "In 1858, the British East India Company established colonial rule in the region, which became part of British India. During this period, the region was divided into provinces, including Punjab, Sindh, Balochistan, and the North-West Frontier Province (now Khyber Pakhtunkhwa).\n",
      "\n",
      "**Pakistan Movement (1930s-1947 CE):**\n",
      "In the early 20th century, the Pakistan Movement, led by Muhammad Ali Jinnah, gained momentum. The movement sought to create a separate homeland for Muslims in British India, where they could live according to their own customs, laws, and values. On August 14, 1947, Pakistan gained independence, with Jinnah as its first Governor-General.\n",
      "\n",
      "**Early Years of Pakistan (1947-1958 CE):**\n",
      "After independence, Pakistan faced numerous challenges, including the migration of millions of Muslims from India, the establishment of a new government and infrastructure, and the integration of various provinces and princely states. The country's first constitution was adopted in 1956, but it was short-lived, as a military coup in 1958 led to the establishment of a military dictatorship.\n",
      "\n",
      "**Military Rule and Democracy (1958-1971 CE):**\n",
      "Pakistan experienced periods of military rule, including the regimes of Ayub Khan (1958-1969) and Yahya Khan (1969-1971). In 1970, general elections were held, which led to the establishment of a democratic government under Zulfikar Ali Bhutto.\n",
      "\n",
      "**Bangladesh Liberation War (1971 CE):**\n",
      "In 1971, East Pakistan (now Bangladesh) declared independence, leading to a brutal civil war. The war ended with the surrender of Pakistani forces and the establishment of Bangladesh as a separate country.\n",
      "\n",
      "**Modern Pakistan (1972 CE-present):**\n",
      "Since 1972, Pakistan has experienced periods of democracy and military rule, with various governments and leaders shaping the country's politics, economy, and society. Today, Pakistan is a nuclear-armed state with a diverse economy, a growing middle class, and a rich cultural heritage.\n",
      "\n",
      "This is a brief overview of Pakistan's history. If you'd like to know more about a specific period or topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0124da8",
   "metadata": {},
   "source": [
    "Because `res` is just another `AIMessage` object, we can append it to `messages`, add another `HumanMessage`, and generate the next response in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f27f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liaquat Ali Khan was the first Prime Minister of Pakistan, serving from August 14, 1947, until his assassination on October 16, 1951. He was a close associate and friend of Muhammad Ali Jinnah, the founder of Pakistan, and played a key role in the country's early years.\n"
     ]
    }
   ],
   "source": [
    "# add latest AI response to messages\n",
    "messages.append(res)\n",
    "\n",
    "# now create a new user prompt\n",
    "prompt=HumanMessage(content=\"Who was the first prime minister of Pakistan?\")\n",
    "\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to LLM\n",
    "res = chat(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96acf04",
   "metadata": {},
   "source": [
    "## Dealing with Hallucinations\n",
    "\n",
    "We have our chatbot, but as mentioned — the knowledge of LLMs can be limited. The reason for this is that LLMs learn all they know during training. An LLM essentially compresses the \"world\" as seen in the training data into the internal parameters of the model. We call this knowledge the _parametric knowledge_ of the model.\n",
    "\n",
    "By default, LLMs have no access to the external world.\n",
    "\n",
    "The result of this is very clear when we ask LLMs about more recent information, like about Deepseek R1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6890df4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liaquat Ali Khan was the first Prime Minister of Pakistan, serving from August 14, 1947, until his assassination on October 16, 1951. He was a close associate and friend of Muhammad Ali Jinnah, the founder of Pakistan, and played a key role in the country's early years.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75275a2",
   "metadata": {},
   "source": [
    "Our chatbot can no longer help us, it doesn't contain the information we need to answer the question. It was very clear that the LLM doesn't know the informaiton, but sometimes an LLM may respond like it _does_ know the answer — and this can be very hard to detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf24cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_knowledge = (\n",
    "    \"Pakistan was created on August 14, 1947, as a result of the partition of British India. \"\n",
    "    \"The movement for independence was led by Muhammad Ali Jinnah and the All-India Muslim League, \"\n",
    "    \"who demanded a separate homeland for Muslims. The new nation consisted of two regions, \"\n",
    "    \"West Pakistan (present-day Pakistan) and East Pakistan (now Bangladesh), separated by about 1,600 km of Indian territory. \"\n",
    "    \"In 1971, East Pakistan broke away after a civil war and became Bangladesh. \"\n",
    "    \"Pakistan's first constitution was adopted in 1956, declaring it an Islamic Republic. \"\n",
    "    \"Since independence, Pakistan has experienced several military coups, democratic transitions, \"\n",
    "    \"and significant developments in its nuclear program, cultural identity, and regional politics.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4407ea",
   "metadata": {},
   "source": [
    "We can feed this additional knowledge into our prompt with some instructions telling the LLM how we'd like it to use this information alongside our original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c14250ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"What is so special about Pakistan?\"\n",
    "\n",
    "augmented_prompt=f\"\"\"Using the context below, answer the query.\n",
    "\n",
    "context:{source_knowledge}\n",
    "Query:{query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac82641",
   "metadata": {},
   "source": [
    "Now we feed this into our chatbot as we were before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d3e6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to GROQ\n",
    "res = chat.invoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ded411f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pakistan is special because it was created as a separate homeland for Muslims, making it a unique nation with a distinct cultural and religious identity. Additionally, it has a complex and fascinating history, having experienced the partition of British India, the separation of East Pakistan (now Bangladesh), and significant developments in its nuclear program, cultural identity, and regional politics.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb13a0",
   "metadata": {},
   "source": [
    "## How do we get this information in the first place?\n",
    "\n",
    "The quality of this answer is phenomenal. This is made possible thanks to the idea of augmented our query with external knowledge (source knowledge). There's just one problem.\n",
    "\n",
    "This is where Pinecone and vector databases comes in place, as they can help us here too. But first, we'll need a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6877a06b",
   "metadata": {},
   "source": [
    "## Importing the Data\n",
    "\n",
    "In this task, we will be importing our data. We will be using the Hugging Face Datasets library to load our data. Specifically, we will be using the `\"jamescalam/deepseek-r1-paper-chunked\"` dataset. This dataset contains the Deepseek R1 paper pre-processed into RAG-ready chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f08a20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'num_tokens', 'pages', 'source'],\n",
       "    num_rows: 76\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset=load_dataset(\n",
    "    \"jamescalam/deepseek-r1-paper-chunked\",\n",
    "    split=\"train\"\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "922e7828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '2501.12948v1',\n",
       " 'chunk-id': 1,\n",
       " 'chunk': \"uestion: If a > 1, then the sum of the real solutions of √a - √a + x = x is equal to Response: <think> To solve the equation √a – √a + x = x, let's start by squaring both . . . (√a-√a+x)² = x² ⇒ a - √a + x = x². Rearrange to isolate the inner square root term:(a – x²)² = a + x ⇒ a² – 2ax² + (x²)² = a + x ⇒ x⁴ - 2ax² - x + (a² – a) = 0\",\n",
       " 'num_tokens': 145,\n",
       " 'pages': [1],\n",
       " 'source': 'https://arxiv.org/abs/2501.12948'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922a80a",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The dataset we are using is sourced from the Deepseek R1 ArXiv papers. Each entry in the dataset represents a \"chunk\" of text from the R1 paper.\n",
    "\n",
    "Because most **L**arge **L**anguage **M**odels (LLMs) only contain knowledge of the world as it was during training, even many of the newest LLMs cannot answer questions about Deepseek R1 — at least not without this data.\n",
    "\n",
    "## Building the Knowledge Base\n",
    "\n",
    "We now have a dataset that can serve as our chatbot knowledge base. Our next task is to transform that dataset into the knowledge base that our chatbot can use. To do this we must use an embedding model and vector database.\n",
    "\n",
    "We begin by initializing our Pinecone client, this requires a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc6281a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "# initialize client\n",
    "pc=Pinecone(api_key=pinecone_api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50b59c4",
   "metadata": {},
   "source": [
    "Delete the old one to save the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dc11197",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name=\"rag1\"\n",
    "# pc.delete_index(index_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec0af063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec, CloudProvider, AwsRegion, Metric\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    metric=Metric.DOTPRODUCT,\n",
    "    dimension=384,\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=CloudProvider.AWS,\n",
    "        region=AwsRegion.US_EAST_1\n",
    "\n",
    "    )\n",
    ")\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ce2a6",
   "metadata": {},
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will use HuggingFace's `sentence-transformers/all-MiniLM-L6-v2` model — we can access it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da194d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d5788",
   "metadata": {},
   "source": [
    "Using this model we can create embeddings like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "410cbb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 384)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326e896",
   "metadata": {},
   "source": [
    "From this we get two (aligning to our two chunks of text) CHANGE-dimensional embeddings.\n",
    "\n",
    "We're now ready to embed and index all our our data! We do this by looping through our dataset and embedding and inserting everything in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54eed657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:36<00:00, 36.53s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "\n",
    "data = dataset.to_pandas()  # this makes it easier to iterate over the dataset\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd980907",
   "metadata": {},
   "source": [
    "We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00fa3bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'dotproduct',\n",
       " 'namespaces': {'': {'vector_count': 76}},\n",
       " 'total_vector_count': 76,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2d360",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation\n",
    "We've built a fully-fledged knowledge base. Now it's time to link that knowledge base to our chatbot. To do that we'll be diving back into LangChain and reusing our template prompt from earlier.\n",
    "\n",
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2146e684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "text_field=\"text\"\n",
    "vectorstore=PineconeVectorStore(\n",
    "    index=index,\n",
    "    embedding=embed_model,\n",
    "    text_key=text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec13686",
   "metadata": {},
   "source": [
    "Using this `vectorstore` we can already query the index and see if we have any relevant information given our question about Llama 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "45bf4699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2501.12948v1-39', metadata={'source': 'https://arxiv.org/abs/2501.12948'}, page_content='## 1.2. Summary of Evaluation Results - **Reasoning tasks:** (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge:** On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 4o on this benchmark.'),\n",
       " Document(id='2501.12948v1-56', metadata={'source': 'https://arxiv.org/abs/2501.12948'}, page_content='Table summary:\\nTable 5 presents a comparison of DeepSeek-R1 distilled models against other leading models across various reasoning-related benchmarks, including AIME 2024, MATH-500, GPQA Diamond, LiveCode Bench, and CodeForces. The metrics evaluated are pass@1 and cons@64. \\nNotably, the DeepSeek-R1-Distill-Qwen-32B model achieves the highest score in AIME 2024 with 72.6, while the DeepSeek-R1-Distill-Llama-70B excels in MATH-500 with 86.7. In GPQA Diamond, the DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-70B models lead with scores of 94.3 and 94.5, respectively. \\nOverall, the DeepSeek-R1 models demonstrate competitive performance, particularly the 14B and 32B variants, which consistently rank high across multiple benchmarks. In contrast, models like GPT-4o-0513 and Claude-3.5-Sonnet-1022 show lower performance, especially in AIME 2024 and MATH-500. This analysis highlights the effectiveness of the DeepSeek-R1 distilled models in reasoning tasks, showcasing their potential for further applications in AI-driven reasoning solutions.\\nTable 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on\\nreasoning-related benchmarks.\\n| Model | AIME 2024 | MATH-500 | GPQA Diamond | LiveCode Bench | CodeForces |\\n| --- | --- | --- | --- | --- | --- |\\n|  | pass@1 | cons@64 | pass@1 | pass@1 | pass@1 |\\n| GPT-4o-0513 | 9.3 | 13.4 | 74.6 | 49.9 | 32.9 |\\n| Claude-3.5-Sonnet-1022 | 16.0 | 26.7 | 78.3 | 65.0 | 38.9 |\\n| OpenAI-o1-mini | 63.6 | 80.0 | 90.0 | 60.0 | 53.8 |\\n| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 | 41.9 |\\n| DeepSeek-R1-Distill-Qwen-1.5B | 28.9 | 52.7 | 83.9 | 33.8 | 16.9 |\\n| DeepSeek-R1-Distill-Qwen-7B | 55.5 | 83.3 | 92.8 | 49.1 | 37.6 |\\n| DeepSeek-R1-Distill-Qwen-14B | 69.7 | 80.0 | 93.9 | 59.1 | 53.1 |\\n| DeepSeek-R1-Distill-Qwen-32B | **72.6** | 83.3 | 94.3 | 62.1 | 57.2 |\\n| DeepSeek-R1-Distill-Llama-8B | 50.4 | 80.0 | 89.1 | 49.0 | 39.6 |\\n| DeepSeek-R1-Distill-Llama-70B | 70.0 | **86.7** | **94.5** | **65.2** | **57.5** |'),\n",
       " Document(id='2501.12948v1-4', metadata={'source': 'https://arxiv.org/abs/2501.12948'}, page_content='Table summary:\\nThe table presents performance metrics for various AI models across different benchmarks, including AIME 2024, Codeforces, GPQA Diamond, MATH-500, and MMLU, measured in accuracy and percentile scores. \\nDeepSeek-R1 shows strong performance with scores of 96.3% and 96.6% at the highest accuracy level (100), while OpenAI-01-1217 achieves 97.3% and 96.4% in the same category. DeepSeek-R1-32B and OpenAI-01-mini exhibit lower scores, particularly in the 80 percentile range, where DeepSeek-V3 performs well with scores of 90.8% and 91.8%. \\nAt lower accuracy levels (60 and 40), DeepSeek models maintain competitive scores, with DeepSeek-R1 recording 72.6% and 39.2% respectively. The table highlights the varying strengths of each model across different tasks, indicating that DeepSeek models generally excel in higher accuracy ranges, while OpenAI models show robust performance in specific benchmarks. This comparative analysis aids in understanding the capabilities and limitations of each AI model in diverse applications.\\n|  | DeepSeek-R1 | OpenAI-01-1217 | DeepSeek-R1-32B | OpenAI-01-mini | DeepSeek-V3 |\\n| --- | --- | --- | --- | --- | --- |\\n|  |  |  |  |  |  |\\n|  |  |  |  |  |  |\\n| 100 |  |  |  |  |  |\\n|  | 96.3 96.6 |  | 97.3 96.4 |  |  |\\n|  |  | 93.4 |  | 94.3 |  |\\n|  |  | 90.6 |  | 90.0 90.2 |  |\\n| 80 | 79.8 79.2 |  |  | 90.8 91.8 | 88.5 |\\n|  |  |  |  |  | 87.4 |\\n|  |  |  |  |  | 85.2 |\\n|  | 72.6 |  |  |  |  |\\n|  |  |  | 75.7 |  |  |\\n|  |  |  | 71.5 |  |  |\\n|  | 63.6 |  | 62.1 |  |  |\\n|  |  |  | 60.0 59.1 |  |  |\\n| 60 |  | 58.7 |  |  |  |\\n| Accuracy / Percentile (%) |  |  |  |  |  |\\n| 40 |  |  |  |  |  |\\n|  | 39.2 |  |  |  |  |\\n| 20 |  |  |  |  |  |\\n| 0 |  |  |  |  |  |\\n|  | AIME 2024 (Pass@1) | Codeforces (Percentile) | GPQA Diamond (Pass@1) | MATH-500 (Pass@1) | MMLU (Pass@1) |\\n|  |  |  |  |  |  |\\n|  |  |  |  |  |  |\\n|  |  |  |  |  |  |')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is so special about Deepseek R1?\"\n",
    "vectorstore.similarity_search(query,k=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a031c4",
   "metadata": {},
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to link the output from our `vectorstore` to our `chat` chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fcca53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query:str):\n",
    "    results=vectorstore.similarity_search(query,k=3)\n",
    "    source_knowledge=\"\\n\".join([x.page_content for x in results])\n",
    "    augmented_prompt=f\"\"\"Using the context below, answer the query.\n",
    "    context:\n",
    "    {source_knowledge}\n",
    "    Query:\n",
    "    {query}\"\"\"\n",
    "    return augmented_prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41281180",
   "metadata": {},
   "source": [
    "Using this we produce an augmented prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf9f1672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the context below, answer the query.\n",
      "    context:\n",
      "    ## 1.2. Summary of Evaluation Results - **Reasoning tasks:** (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly surpassing OpenAI-01-1217. On MATH-500, it attains an impressive score of 97.3%, performing on par with OpenAI-01-1217 and significantly outperforming other models. (2) On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks, as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than DeepSeek-V3, which could help developers in real world tasks. - **Knowledge:** On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performance is slightly below that of OpenAI-01-1217 on these benchmarks, DeepSeek-R1 surpasses other closed-source models, demonstrating its competitive edge in educational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstrating its capability in handling fact-based queries. A similar trend is observed where OpenAI-01 surpasses 4o on this benchmark.\n",
      "Table summary:\n",
      "Table 5 presents a comparison of DeepSeek-R1 distilled models against other leading models across various reasoning-related benchmarks, including AIME 2024, MATH-500, GPQA Diamond, LiveCode Bench, and CodeForces. The metrics evaluated are pass@1 and cons@64. \n",
      "Notably, the DeepSeek-R1-Distill-Qwen-32B model achieves the highest score in AIME 2024 with 72.6, while the DeepSeek-R1-Distill-Llama-70B excels in MATH-500 with 86.7. In GPQA Diamond, the DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-70B models lead with scores of 94.3 and 94.5, respectively. \n",
      "Overall, the DeepSeek-R1 models demonstrate competitive performance, particularly the 14B and 32B variants, which consistently rank high across multiple benchmarks. In contrast, models like GPT-4o-0513 and Claude-3.5-Sonnet-1022 show lower performance, especially in AIME 2024 and MATH-500. This analysis highlights the effectiveness of the DeepSeek-R1 distilled models in reasoning tasks, showcasing their potential for further applications in AI-driven reasoning solutions.\n",
      "Table 5 | Comparison of DeepSeek-R1 distilled models and other comparable models on\n",
      "reasoning-related benchmarks.\n",
      "| Model | AIME 2024 | MATH-500 | GPQA Diamond | LiveCode Bench | CodeForces |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "|  | pass@1 | cons@64 | pass@1 | pass@1 | pass@1 |\n",
      "| GPT-4o-0513 | 9.3 | 13.4 | 74.6 | 49.9 | 32.9 |\n",
      "| Claude-3.5-Sonnet-1022 | 16.0 | 26.7 | 78.3 | 65.0 | 38.9 |\n",
      "| OpenAI-o1-mini | 63.6 | 80.0 | 90.0 | 60.0 | 53.8 |\n",
      "| QwQ-32B-Preview | 50.0 | 60.0 | 90.6 | 54.5 | 41.9 |\n",
      "| DeepSeek-R1-Distill-Qwen-1.5B | 28.9 | 52.7 | 83.9 | 33.8 | 16.9 |\n",
      "| DeepSeek-R1-Distill-Qwen-7B | 55.5 | 83.3 | 92.8 | 49.1 | 37.6 |\n",
      "| DeepSeek-R1-Distill-Qwen-14B | 69.7 | 80.0 | 93.9 | 59.1 | 53.1 |\n",
      "| DeepSeek-R1-Distill-Qwen-32B | **72.6** | 83.3 | 94.3 | 62.1 | 57.2 |\n",
      "| DeepSeek-R1-Distill-Llama-8B | 50.4 | 80.0 | 89.1 | 49.0 | 39.6 |\n",
      "| DeepSeek-R1-Distill-Llama-70B | 70.0 | **86.7** | **94.5** | **65.2** | **57.5** |\n",
      "Table summary:\n",
      "The table presents performance metrics for various AI models across different benchmarks, including AIME 2024, Codeforces, GPQA Diamond, MATH-500, and MMLU, measured in accuracy and percentile scores. \n",
      "DeepSeek-R1 shows strong performance with scores of 96.3% and 96.6% at the highest accuracy level (100), while OpenAI-01-1217 achieves 97.3% and 96.4% in the same category. DeepSeek-R1-32B and OpenAI-01-mini exhibit lower scores, particularly in the 80 percentile range, where DeepSeek-V3 performs well with scores of 90.8% and 91.8%. \n",
      "At lower accuracy levels (60 and 40), DeepSeek models maintain competitive scores, with DeepSeek-R1 recording 72.6% and 39.2% respectively. The table highlights the varying strengths of each model across different tasks, indicating that DeepSeek models generally excel in higher accuracy ranges, while OpenAI models show robust performance in specific benchmarks. This comparative analysis aids in understanding the capabilities and limitations of each AI model in diverse applications.\n",
      "|  | DeepSeek-R1 | OpenAI-01-1217 | DeepSeek-R1-32B | OpenAI-01-mini | DeepSeek-V3 |\n",
      "| --- | --- | --- | --- | --- | --- |\n",
      "|  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |\n",
      "| 100 |  |  |  |  |  |\n",
      "|  | 96.3 96.6 |  | 97.3 96.4 |  |  |\n",
      "|  |  | 93.4 |  | 94.3 |  |\n",
      "|  |  | 90.6 |  | 90.0 90.2 |  |\n",
      "| 80 | 79.8 79.2 |  |  | 90.8 91.8 | 88.5 |\n",
      "|  |  |  |  |  | 87.4 |\n",
      "|  |  |  |  |  | 85.2 |\n",
      "|  | 72.6 |  |  |  |  |\n",
      "|  |  |  | 75.7 |  |  |\n",
      "|  |  |  | 71.5 |  |  |\n",
      "|  | 63.6 |  | 62.1 |  |  |\n",
      "|  |  |  | 60.0 59.1 |  |  |\n",
      "| 60 |  | 58.7 |  |  |  |\n",
      "| Accuracy / Percentile (%) |  |  |  |  |  |\n",
      "| 40 |  |  |  |  |  |\n",
      "|  | 39.2 |  |  |  |  |\n",
      "| 20 |  |  |  |  |  |\n",
      "| 0 |  |  |  |  |  |\n",
      "|  | AIME 2024 (Pass@1) | Codeforces (Percentile) | GPQA Diamond (Pass@1) | MATH-500 (Pass@1) | MMLU (Pass@1) |\n",
      "|  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |\n",
      "|  |  |  |  |  |  |\n",
      "    Query:\n",
      "    What is so special about Deepseek R1?\n"
     ]
    }
   ],
   "source": [
    "print(augment_prompt(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3e2dc",
   "metadata": {},
   "source": [
    "There is still a lot of text here, so let's pass it onto our chat model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8090d9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-R1 is special because it achieves outstanding results in various reasoning-related benchmarks, such as AIME 2024, MATH-500, and GPQA Diamond, demonstrating its competitive edge in educational tasks. It also performs exceptionally well in coding-related tasks, achieving an expert level in code competition tasks with a 2,029 Elo rating on Codeforces, outperforming 96.3% of human participants. Additionally, DeepSeek-R1 shows strong performance in knowledge-based tasks, significantly outperforming other models in benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond. Its capabilities make it a potential solution for AI-driven reasoning applications.\n"
     ]
    }
   ],
   "source": [
    "prompt=HumanMessage(content=augment_prompt(query))\n",
    "messages.append(prompt)\n",
    "res=chat(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00bcf09",
   "metadata": {},
   "source": [
    "We can continue with another Deepseek R1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2e794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There is no mention of \"DeepSeek R1 Zero\" in the provided context. The context only compares DeepSeek-R1 with other models such as OpenAI-01-1217, DeepSeek-V3, GPT-4o-0513, and Claude-3.5-Sonnet-1022, but does not mention a \"DeepSeek R1 Zero\" model. Therefore, it is not possible to compare DeepSeek R1 with DeepSeek R1 Zero based on the provided information.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 104, 'prompt_tokens': 4582, 'total_tokens': 4686, 'completion_time': 0.297127955, 'prompt_time': 0.38954781, 'queue_time': 0.04457126, 'total_time': 0.686675765}, 'model_name': 'Llama-3.3-70B-Versatile', 'system_fingerprint': 'fp_34d416ee39', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--26355c11-c7bd-450a-8385-5f323c03f635-0', usage_metadata={'input_tokens': 4582, 'output_tokens': 104, 'total_tokens': 4686})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augment_prompt(\n",
    "        \"how does deepseek r1 compare to deepseek r1 zero?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "res = chat(messages + [prompt])\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31764ac2",
   "metadata": {},
   "source": [
    "You can continue asking questions about Deepseek R1, but once you're done you can delete the index to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e53a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d133d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fellowship_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
