{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5161fe",
   "metadata": {},
   "source": [
    "# 1. Imports and Environment Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60481ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api = os.getenv(\"FELLOWSHIP_GROQ_KEY\")\n",
    "pinecone_api = os.getenv(\"PINECONE_API_KEY\")\n",
    "pdf_path = \"text.pdf\"\n",
    "index_name = \"pakistan-history\"\n",
    "dimension = 384\n",
    "\n",
    "if not pinecone_api:\n",
    "    raise ValueError(\"Set PINECONE_API_KEY in your .env\")\n",
    "if not groq_api:\n",
    "    raise ValueError(\"Set FELLOWSHIP_GROQ_KEY in your .env\")\n",
    "if not os.path.exists(pdf_path):\n",
    "    raise ValueError(f\"PDF file not found: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb246c",
   "metadata": {},
   "source": [
    "# 2. Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed51fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import SystemMessage, AIMessage, HumanMessage\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import AwsRegion, CloudProvider, Pinecone, ServerlessSpec, Metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3496d300",
   "metadata": {},
   "source": [
    "# 3. Initialize LLM (Groq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67367798",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat=ChatGroq(\n",
    "    groq_api_key=groq_api,\n",
    "    model_name=\"Llama-3.3-70B-Versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad436d2",
   "metadata": {},
   "source": [
    "# 4. Load and Split PDF into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f05ed835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and splitting PDF...\n",
      "Loaded and split PDF into 3 chunks.\n"
     ]
    }
   ],
   "source": [
    "def load_pdf_split(path, chunk_size=500, chunk_overlap=100):\n",
    "    text=\"\"\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            text+=page.get_text(\"text\") + \"\\n\"\n",
    "    splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=chunk_overlap)\n",
    "    chunks=splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "print(\"Loading and splitting PDF...\")\n",
    "chunks=load_pdf_split(pdf_path)\n",
    "data=pd.DataFrame({\"chunks\":chunks})\n",
    "print(f\"Loaded and split PDF into {len(data)} chunks.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63710b5",
   "metadata": {},
   "source": [
    "# 5. Pinecone Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab4125c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=pinecone_api)\n",
    "index_name=\"pakistan-history\"\n",
    "# Delete old index if exists\n",
    "if index_name in [i[\"name\"] for i in pc.list_indexes()]:\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "# Create new index\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    metric=Metric.DOTPRODUCT,\n",
    "    dimension=dimension,\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=CloudProvider.AWS,\n",
    "        region=AwsRegion.US_EAST_1\n",
    "    )\n",
    ")\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42d9f6",
   "metadata": {},
   "source": [
    "# 6. Embedding Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "61f9ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd4ecf",
   "metadata": {},
   "source": [
    "# 7. Upload Data to Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "541d76b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if index has no vectors, upload; otherwise skip to avoid duplicates.\n",
    "stats=index.describe_index_stats()\n",
    "total_vectors = stats.get(\"namespaces\", {}).get(\"\", {}).get(\"vector_count\", 0) if stats else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index is empty — uploading chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if total_vectors == 0:\n",
    "    print(\"Index is empty — uploading chunks...\")\n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        i_end = min(len(data), i + batch_size)\n",
    "        batch = data.iloc[i:i_end]\n",
    "        ids = [f\"chunk-{j}\" for j in range(i, i_end)]\n",
    "        texts = batch[\"chunks\"].tolist()\n",
    "        embeds = embed_model.embed_documents(texts)\n",
    "        metadata = [{\"text\": t} for t in texts]\n",
    "        # Pinecone expects iterable of (id, vector, metadata)\n",
    "        index.upsert(vectors=zip(ids, embeds, metadata))\n",
    "    print(\"Upload complete.\")\n",
    "else:\n",
    "    print(f\"Index already contains vectors ({total_vectors}) — skipping upload.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf74032",
   "metadata": {},
   "source": [
    "# 8. VectorStore and Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef9cc4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = \"text\" \n",
    "\n",
    "vectorstore=PineconeVectorStore(\n",
    "    index=index, \n",
    "    embedding=embed_model,\n",
    "    text_key=\"text\"\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2abbe8",
   "metadata": {},
   "source": [
    "# 9. Chat Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06bb03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessage(\n",
    "    content=\"You are a helpful assistant. Use the provided document context to answer user queries. \"\n",
    "            \"If the answer is not in the context, say you don't know.\"\n",
    ")\n",
    "message_history = [system_prompt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed38eff",
   "metadata": {},
   "source": [
    "# 10. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "659f6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_for_query(query: str, k: int = 3) -> str:\n",
    "    docs = retriever.get_relevant_documents(query) \n",
    "    # docs = vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "    texts = [d.page_content for d in docs]\n",
    "    return \"\\n\\n\".join(texts)\n",
    "\n",
    "def build_user_prompt_with_context(query: str) -> HumanMessage:\n",
    "    context = get_context_for_query(query)\n",
    "    augmented = (\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question:\\n{query}\\n\\n\"\n",
    "        \"Answer using ONLY the context above. \"\n",
    "        \"If the context does not contain the answer, say 'I don't know from the provided documents.'\"\n",
    "    )\n",
    "    return HumanMessage(content=augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301777e",
   "metadata": {},
   "source": [
    "# 11. Conversational Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58cfc38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversational RAG Chatbot ready!\n",
      "Ask anything about the PDF. Type 'exit' or 'quit' to stop.\n",
      "\n",
      "Bot: Liaquat Ali Khan became the first Prime Minister of Pakistan.\n",
      "Bot: Pakistan was created on August 14, 1947, after the partition of British India. The country experienced periods of democracy and military rule throughout its history. In 1956, Pakistan adopted its first constitution, officially becoming the Islamic Republic of Pakistan. Today, Pakistan is known for its rich culture, strong military, and significant role in South Asian politics. Pakistan faced major challenges from the beginning, including a shortage of resources, migration of millions of refugees, and administrative difficulties. Despite these hardships, Pakistan quickly established its government institutions.\n",
      "Goodbye\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConversational RAG Chatbot ready!\")\n",
    "print(\"Ask anything about the PDF. Type 'exit' or 'quit' to stop.\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \").strip()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nExiting...\")\n",
    "        break\n",
    "\n",
    "    if not user_input:\n",
    "        continue\n",
    "    if user_input.lower() in (\"exit\", \"quit\", \"bye\"):\n",
    "        print(\"Goodbye\")\n",
    "        break\n",
    "\n",
    "    user_msg = build_user_prompt_with_context(user_input)\n",
    "    message_history.append(user_msg)\n",
    "\n",
    "    # send to Groq model\n",
    "    response = chat(message_history)\n",
    "    bot_text = response.content if hasattr(response, \"content\") else str(response)\n",
    "\n",
    "    print(\"Bot:\", bot_text)\n",
    "    message_history.append(AIMessage(content=bot_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fellowship_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
